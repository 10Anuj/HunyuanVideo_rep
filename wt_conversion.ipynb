{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d4a1a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model state dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2945217/4011261502.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  original_state_dict = torch.load(ORIGINAL_MODEL_PATH, map_location='cpu')['module']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting DoubleStreamBlocks...\n",
      "Converting SingleStreamBlocks...\n",
      "Saving new refactored state dictionary to ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states_refactored.pt...\n",
      "Conversion complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import collections\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the original downloaded model checkpoint\n",
    "ORIGINAL_MODEL_PATH = \"ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt\"\n",
    "# Path where the new, refactored model will be saved\n",
    "NEW_MODEL_PATH = \"ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states_refactored.pt\"\n",
    "NUM_DOUBLE_BLOCKS = 20\n",
    "# We will need to do the same for single blocks later\n",
    "NUM_SINGLE_BLOCKS = 40 \n",
    "\n",
    "def convert_weights():\n",
    "    \"\"\"\n",
    "    Loads the original HunyuanVideo weights, splits the fused QKV layers,\n",
    "    and saves a new state dictionary compatible with the refactored model.\n",
    "    \"\"\"\n",
    "    print(\"Loading original model state dictionary...\")\n",
    "    # We load the 'module' part of the checkpoint\n",
    "    original_state_dict = torch.load(ORIGINAL_MODEL_PATH, map_location='cpu')['module']\n",
    "    new_state_dict = original_state_dict.copy()\n",
    "\n",
    "    print(\"Converting DoubleStreamBlocks...\")\n",
    "    for i in range(NUM_DOUBLE_BLOCKS):\n",
    "        # --- Process Image Attention Weights ---\n",
    "        img_qkv_weight_key = f\"double_blocks.{i}.img_attn_qkv.weight\"\n",
    "        img_qkv_bias_key = f\"double_blocks.{i}.img_attn_qkv.bias\"\n",
    "\n",
    "        if img_qkv_weight_key in original_state_dict:\n",
    "            # Get the combined QKV weight and bias\n",
    "            qkv_weight = original_state_dict[img_qkv_weight_key]\n",
    "            qkv_bias = original_state_dict[img_qkv_bias_key]\n",
    "\n",
    "            # Split them into three equal parts\n",
    "            q_weight, k_weight, v_weight = torch.chunk(qkv_weight, 3, dim=0)\n",
    "            q_bias, k_bias, v_bias = torch.chunk(qkv_bias, 3, dim=0)\n",
    "\n",
    "            # Add the new weights to our new state dict with new names\n",
    "            new_state_dict[f\"double_blocks.{i}.img_attn_q.weight\"] = q_weight\n",
    "            new_state_dict[f\"double_blocks.{i}.img_attn_k.weight\"] = k_weight\n",
    "            new_state_dict[f\"double_blocks.{i}.img_attn_v.weight\"] = v_weight\n",
    "            new_state_dict[f\"double_blocks.{i}.img_attn_q.bias\"] = q_bias\n",
    "            new_state_dict[f\"double_blocks.{i}.img_attn_k.bias\"] = k_bias\n",
    "            new_state_dict[f\"double_blocks.{i}.img_attn_v.bias\"] = v_bias\n",
    "\n",
    "            # Remove the old, combined keys\n",
    "            del new_state_dict[img_qkv_weight_key]\n",
    "            del new_state_dict[img_qkv_bias_key]\n",
    "\n",
    "        # --- Process Text Attention Weights (Identical Logic) ---\n",
    "        txt_qkv_weight_key = f\"double_blocks.{i}.txt_attn_qkv.weight\"\n",
    "        txt_qkv_bias_key = f\"double_blocks.{i}.txt_attn_qkv.bias\"\n",
    "        \n",
    "        if txt_qkv_weight_key in original_state_dict:\n",
    "            qkv_weight = original_state_dict[txt_qkv_weight_key]\n",
    "            qkv_bias = original_state_dict[txt_qkv_bias_key]\n",
    "\n",
    "            q_weight, k_weight, v_weight = torch.chunk(qkv_weight, 3, dim=0)\n",
    "            q_bias, k_bias, v_bias = torch.chunk(qkv_bias, 3, dim=0)\n",
    "\n",
    "            new_state_dict[f\"double_blocks.{i}.txt_attn_q.weight\"] = q_weight\n",
    "            new_state_dict[f\"double_blocks.{i}.txt_attn_k.weight\"] = k_weight\n",
    "            new_state_dict[f\"double_blocks.{i}.txt_attn_v.weight\"] = v_weight\n",
    "            new_state_dict[f\"double_blocks.{i}.txt_attn_q.bias\"] = q_bias\n",
    "            new_state_dict[f\"double_blocks.{i}.txt_attn_k.bias\"] = k_bias\n",
    "            new_state_dict[f\"double_blocks.{i}.txt_attn_v.bias\"] = v_bias\n",
    "\n",
    "            del new_state_dict[txt_qkv_weight_key]\n",
    "            del new_state_dict[txt_qkv_bias_key]\n",
    "\n",
    "    # (We would add a similar loop for single_blocks here)\n",
    "    # In your convert_weights.py script...\n",
    "\n",
    "    # (Add this loop after the DoubleStreamBlocks loop)\n",
    "\n",
    "    print(\"Converting SingleStreamBlocks...\")\n",
    "    for i in range(NUM_SINGLE_BLOCKS): # Make sure NUM_SINGLE_BLOCKS is defined, e.g., 40\n",
    "        key_prefix = f\"single_blocks.{i}.\"\n",
    "        linear1_weight_key = f\"{key_prefix}linear1.weight\"\n",
    "        linear1_bias_key = f\"{key_prefix}linear1.bias\"\n",
    "\n",
    "        if linear1_weight_key in original_state_dict:\n",
    "            # Get the combined QKV+MLP weight and bias\n",
    "            linear1_weight = original_state_dict[linear1_weight_key]\n",
    "            linear1_bias = original_state_dict[linear1_bias_key]\n",
    "\n",
    "            # The first part is for QKV, the second is for the MLP\n",
    "            hidden_size = 3072 # As defined in the model\n",
    "            mlp_hidden_dim = 12288 # hidden_size * 4\n",
    "\n",
    "            # Split the weight tensor into QKV and MLP parts\n",
    "            qkv_weight, mlp_weight = torch.split(linear1_weight, [hidden_size * 3, mlp_hidden_dim], dim=0)\n",
    "            qkv_bias, mlp_bias = torch.split(linear1_bias, [hidden_size * 3, mlp_hidden_dim], dim=0)\n",
    "            \n",
    "            # Further split the QKV part into Q, K, and V\n",
    "            q_weight, k_weight, v_weight = torch.chunk(qkv_weight, 3, dim=0)\n",
    "            q_bias, k_bias, v_bias = torch.chunk(qkv_bias, 3, dim=0)\n",
    "\n",
    "            # Add the four new weights and biases to our new state dict\n",
    "            new_state_dict[f\"{key_prefix}q_proj.weight\"] = q_weight\n",
    "            new_state_dict[f\"{key_prefix}k_proj.weight\"] = k_weight\n",
    "            new_state_dict[f\"{key_prefix}v_proj.weight\"] = v_weight\n",
    "            new_state_dict[f\"{key_prefix}mlp_proj.weight\"] = mlp_weight\n",
    "            \n",
    "            new_state_dict[f\"{key_prefix}q_proj.bias\"] = q_bias\n",
    "            new_state_dict[f\"{key_prefix}k_proj.bias\"] = k_bias\n",
    "            new_state_dict[f\"{key_prefix}v_proj.bias\"] = v_bias\n",
    "            new_state_dict[f\"{key_prefix}mlp_proj.bias\"] = mlp_bias\n",
    "\n",
    "            # Remove the old, combined key\n",
    "            del new_state_dict[linear1_weight_key]\n",
    "            del new_state_dict[linear1_bias_key]\n",
    "\n",
    "    print(f\"Saving new refactored state dictionary to {NEW_MODEL_PATH}...\")\n",
    "    torch.save({'module': new_state_dict}, NEW_MODEL_PATH)\n",
    "    print(\"Conversion complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    convert_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c1a070d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved recompute_schedule_k.pt and recompute_schedule_v.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "recompute_schedule_k = {'double_0': [0, 6, 12],\n",
    " 'double_1': [0, 4, 11, 14],\n",
    " 'double_2': [0, 1, 5, 11],\n",
    " 'double_3': [0, 1, 2, 4, 7, 10, 13, 14],\n",
    " 'double_4': [0, 1, 2, 4, 8, 12, 14],\n",
    " 'double_5': [0, 1, 2, 3, 5, 8, 11, 13, 14],\n",
    " 'double_6': [0, 1, 2, 3, 5, 8, 11, 13, 14],\n",
    " 'double_7': [0, 1, 2, 3, 5, 8, 11, 13, 14],\n",
    " 'double_8': [0, 1, 2, 3, 4, 6, 9, 12, 13, 14],\n",
    " 'double_9': [0, 1, 2, 3, 4, 6, 9, 12, 13, 14],\n",
    " 'double_10': [0, 1, 2, 3, 4, 6, 9, 12, 13, 14],\n",
    " 'double_11': [0, 1, 2, 3, 4, 5, 7, 9, 11, 12, 13, 14],\n",
    " 'double_12': [0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 13, 14],\n",
    " 'double_13': [0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 13, 14],\n",
    " 'double_14': [0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 13, 14],\n",
    " 'double_15': [0, 1, 2, 3, 4, 5, 6, 7, 9, 11, 12, 13, 14],\n",
    " 'double_16': [0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 13, 14],\n",
    " 'double_17': [0, 1, 2, 3, 4, 5, 7, 9, 11, 13, 14],\n",
    " 'double_18': [0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 13, 14],\n",
    " 'double_19': [0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 13, 14],\n",
    " 'single_0': [0, 1, 2, 3, 4, 5, 6, 7, 9, 11, 12, 13, 14],\n",
    " 'single_1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_2': [0, 1, 2, 3, 4, 5, 6, 7, 9, 11, 12, 13, 14],\n",
    " 'single_3': [0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 13, 14],\n",
    " 'single_4': [0, 1, 2, 3, 4, 5, 7, 10, 12, 13, 14],\n",
    " 'single_5': [0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 13, 14],\n",
    " 'single_6': [0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 13, 14],\n",
    " 'single_7': [0, 1, 2, 3, 4, 5, 7, 10, 12, 14],\n",
    " 'single_8': [0, 1, 2, 3, 4, 5, 7, 10, 12, 13, 14],\n",
    " 'single_9': [0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 13, 14],\n",
    " 'single_10': [0, 1, 2, 3, 4, 5, 6, 7, 9, 11, 12, 13, 14],\n",
    " 'single_11': [0, 1, 2, 3, 4, 5, 7, 10, 12, 13, 14],\n",
    " 'single_12': [0, 1, 2, 3, 4, 5, 7, 10, 12, 14],\n",
    " 'single_13': [0, 1, 2, 3, 4, 5, 7, 10, 13, 14],\n",
    " 'single_14': [0, 1, 2, 3, 4, 5, 7, 10, 12, 14],\n",
    " 'single_15': [0, 1, 2, 3, 4, 6, 9, 12, 14],\n",
    " 'single_16': [0, 1, 2, 3, 4, 5, 7, 10, 13, 14],\n",
    " 'single_17': [0, 1, 2, 3, 4, 5, 7, 10, 13, 14],\n",
    " 'single_18': [0, 1, 2, 3, 5, 9, 13],\n",
    " 'single_19': [0, 1, 2, 3, 4, 5, 7, 10, 13, 14],\n",
    " 'single_20': [0, 1, 2, 3, 4, 6, 10, 13],\n",
    " 'single_21': [0, 1, 2, 3, 4, 6, 10, 13],\n",
    " 'single_22': [0, 1, 2, 3, 4, 6, 10, 13],\n",
    " 'single_23': [0, 1, 2, 3, 5, 9, 13],\n",
    " 'single_24': [0, 1, 2, 3, 4, 6, 10, 14],\n",
    " 'single_25': [0, 1, 3, 10],\n",
    " 'single_26': [0, 2, 14],\n",
    " 'single_27': [0, 12],\n",
    " 'single_28': [0, 1, 3, 11],\n",
    " 'single_29': [0, 1, 2, 3, 5, 10, 14],\n",
    " 'single_30': [0, 1, 2, 4, 9, 14],\n",
    " 'single_31': [0, 1, 2, 3, 4, 5, 7, 11, 13, 14],\n",
    " 'single_32': [0, 1, 2, 3, 6, 12],\n",
    " 'single_33': [0, 1, 2, 3, 5, 10, 14],\n",
    " 'single_34': [0, 1, 2, 6, 14],\n",
    " 'single_35': [0, 1, 3, 11],\n",
    " 'single_36': [0, 1, 3, 12],\n",
    " 'single_37': [0, 1, 2, 7],\n",
    " 'single_38': [0, 1, 6, 14],\n",
    " 'single_39': [0, 1, 11]}\n",
    "\n",
    "recompute_schedule_v = {'double_0': [0, 2, 7, 11, 13, 14],\n",
    " 'double_1': [0, 1, 4, 8, 12, 14],\n",
    " 'double_2': [0, 1, 2, 4, 7, 11, 13, 14],\n",
    " 'double_3': [0, 1, 2, 3, 5, 7, 10, 12, 14],\n",
    " 'double_4': [0, 1, 2, 3, 5, 7, 10, 12, 13, 14],\n",
    " 'double_5': [0, 1, 2, 3, 4, 5, 7, 9, 11, 12, 13, 14],\n",
    " 'double_6': [0, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14],\n",
    " 'double_7': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14],\n",
    " 'double_8': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'double_9': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'double_10': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'double_11': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'double_12': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'double_13': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'double_14': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'double_15': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'double_16': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'double_17': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'double_18': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'double_19': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_0': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_3': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_4': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_5': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_6': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_7': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_8': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_9': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_10': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_11': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_12': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_13': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_14': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_15': [0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 13, 14],\n",
    " 'single_16': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_17': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_18': [0, 1, 2, 3, 4, 5, 6, 7, 9, 11, 13, 14],\n",
    " 'single_19': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    " 'single_20': [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 13, 14],\n",
    " 'single_21': [0, 1, 2, 3, 4, 5, 7, 10, 13, 14],\n",
    " 'single_22': [0, 1, 2, 3, 4, 5, 6, 7, 9, 11, 13, 14],\n",
    " 'single_23': [0, 1, 2, 3, 4, 5, 6, 8, 11, 13, 14],\n",
    " 'single_24': [0, 1, 2, 3, 4, 5, 7, 11, 14],\n",
    " 'single_25': [0, 1, 2, 4, 9, 14],\n",
    " 'single_26': [0, 1, 2, 4, 9, 14],\n",
    " 'single_27': [0, 5],\n",
    " 'single_28': [0, 1, 2, 3, 5, 10, 14],\n",
    " 'single_29': [0, 1, 2, 3, 4, 5, 8, 12, 14],\n",
    " 'single_30': [0, 1, 2, 3, 4, 5, 8, 12, 14],\n",
    " 'single_31': [0, 1, 2, 3, 4, 5, 6, 9, 12, 14],\n",
    " 'single_32': [0, 1, 2, 3, 4, 5, 8, 12, 14],\n",
    " 'single_33': [0, 1, 2, 3, 4, 5, 7, 11, 14],\n",
    " 'single_34': [0, 1, 2, 3, 7, 12, 14],\n",
    " 'single_35': [0, 1, 2, 3, 8, 14],\n",
    " 'single_36': [0, 1, 2, 3, 4, 5, 9, 14],\n",
    " 'single_37': [0, 1, 2, 3, 4, 5, 8, 13],\n",
    " 'single_38': [0, 1, 6],\n",
    " 'single_39': [0, 1, 2, 5, 13]}\n",
    "\n",
    "\n",
    "# Save both dictionaries as .pt files\n",
    "torch.save(recompute_schedule_k, \"recompute_schedule_k.pt\")\n",
    "torch.save(recompute_schedule_v, \"recompute_schedule_v.pt\")\n",
    "\n",
    "print(\"✅ Saved recompute_schedule_k.pt and recompute_schedule_v.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbf03aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2945217/2978729729.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(\"ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt\", map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['module'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "state = torch.load(\"ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt\", map_location='cpu')\n",
    "print(state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d899b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HunyuanVideo_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
